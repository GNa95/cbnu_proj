{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e548563-cec1-4ceb-a85e-d6330c30f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlit/miniconda3/envs/pgn/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# ===============================\n",
    "# 1Ô∏è‚É£ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú Î∞è ÎùºÎ≤®Î≥Ñ Î¶¨Ïä§Ìä∏\n",
    "# ===============================\n",
    "data_dir = \"/mnt/nas/seohan_image/seohan_data/origin/\"\n",
    "labels = ['27_45','27_30']\n",
    "\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "\n",
    "for label in labels:\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    for img_name in os.listdir(label_dir):\n",
    "            image_paths.append(os.path.join(label_dir, img_name))\n",
    "            image_labels.append(label)\n",
    "\n",
    "# ===============================\n",
    "# 2Ô∏è‚É£ train/val ÎÇòÎàÑÍ∏∞ (80:20)\n",
    "# ===============================\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    image_paths, image_labels, test_size=0.2, stratify=image_labels, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa3b83b-88c1-4493-8b41-6b95f230d6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlit/miniconda3/envs/pgn/lib/python3.9/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.08, scale_limit=0.05, rotate_limit=10, border_mode=cv2.BORDER_REFLECT_101, p=0.8\n",
    "    ),\n",
    "    A.MotionBlur(blur_limit=5, p=0.3),\n",
    "    A.GaussianBlur(blur_limit=3, p=0.2),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.3),  # ÏÉâÏò®ÎèÑ Î≥ÄÌôî ÎåÄÏùë\n",
    "    A.CLAHE(p=0.2),  # Ï°∞Î™Ö Î≥¥Ï†ï\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.Resize(299, 299),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(299, 299),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defcda8a-8694-4884-ab76-4f7bc6bd1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.label_to_idx = {l: i for i, l in enumerate(sorted(set(labels)))}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n",
    "        label = self.label_to_idx[self.labels[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image, label\n",
    "\n",
    "train_dataset = CustomDataset(train_paths, train_labels, transform=train_transform)\n",
    "val_dataset = CustomDataset(val_paths, val_labels, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ba6ec8-5f99-4550-be3b-0da4bd3a358c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.models' has no attribute 'inception_resnet_v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minception_resnet_v2\u001b[49m(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMAGENET1K_V1\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# ÏµúÏã† torchvision Í∏∞Ï§Ä\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# ÎùºÎ≤® 2Í∞ú\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Ï¥àÍ∏∞Ïóî feature extractor ÎèôÍ≤∞\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.models' has no attribute 'inception_resnet_v2'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.inception_resnet_v2(weights='IMAGENET1K_V1')  # ÏµúÏã† torchvision Í∏∞Ï§Ä\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # ÎùºÎ≤® 2Í∞ú\n",
    "\n",
    "# Ï¥àÍ∏∞Ïóî feature extractor ÎèôÍ≤∞\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29810dfe-df08-473a-8879-ba710ededd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===============================\n",
    "# (1) ÎùºÎ≤® Îç∞Ïù¥ÌÑ∞Î°ú class weight Í≥ÑÏÇ∞\n",
    "# ===============================\n",
    "# train_labels: Î¨∏ÏûêÏó¥ Î¶¨Ïä§Ìä∏ Ïòà) [\"label1\", \"label2\", \"label2\", ...]\n",
    "label_to_idx = {'label1': 0, 'label2': 1}\n",
    "y_train = np.array([label_to_idx[l] for l in train_labels])\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# ===============================\n",
    "# (2) ÏÜêÏã§ Ìï®Ïàò Ï†ïÏùò Ïãú weight Ï†ÅÏö©\n",
    "# ===============================\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8a49f-0f75-4dac-a9be-d204ab137243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta_predict(model, image, n=5):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for _ in range(n):\n",
    "        aug_img = train_transform(image=image)['image'].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            preds.append(torch.softmax(model(aug_img), dim=1).cpu().numpy())\n",
    "    return np.mean(preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28cc59-6d05-41f1-a6d8-c69526b03355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf376000-85e7-422c-bc00-479c024567d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Class Weights: tensor([0.6983, 1.7610], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlit/miniconda3/envs/pgn/lib/python3.9/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "[Epoch 1] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [39:23<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [1/20] Train Loss: 0.0287, Acc: 98.97% | Val Loss: 0.0003, Acc: 100.00%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_000_val_100.00.pt\n",
      "üåü ÏµúÍ≥† ÏÑ±Îä• Í∞±Ïã†! Best Acc: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [38:39<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [2/20] Train Loss: 0.0047, Acc: 99.86% | Val Loss: 0.0036, Acc: 99.99%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_001_val_99.99.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [38:31<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [3/20] Train Loss: 0.0048, Acc: 99.87% | Val Loss: 0.0058, Acc: 99.76%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_002_val_99.76.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [36:59<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [4/20] Train Loss: 0.0033, Acc: 99.90% | Val Loss: 0.0013, Acc: 99.96%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_003_val_99.96.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [37:51<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [5/20] Train Loss: 0.0015, Acc: 99.96% | Val Loss: 0.0020, Acc: 99.96%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_004_val_99.96.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [44:10<00:00,  2.27s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [6/20] Train Loss: 0.0019, Acc: 99.97% | Val Loss: 0.0005, Acc: 99.99%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_005_val_99.99.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [44:37<00:00,  2.29s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [7/20] Train Loss: 0.0015, Acc: 99.95% | Val Loss: 0.0003, Acc: 99.99%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_006_val_99.99.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [44:12<00:00,  2.27s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [8/20] Train Loss: 0.0006, Acc: 99.98% | Val Loss: 0.0003, Acc: 99.98%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_007_val_99.98.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [43:59<00:00,  2.26s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [9/20] Train Loss: 0.0012, Acc: 99.97% | Val Loss: 0.0003, Acc: 99.97%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_008_val_99.97.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [40:24<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [10/20] Train Loss: 0.0010, Acc: 99.97% | Val Loss: 0.0002, Acc: 99.98%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_009_val_99.98.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [36:36<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [11/20] Train Loss: 0.0001, Acc: 100.00% | Val Loss: 0.0000, Acc: 100.00%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_010_val_100.00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [36:04<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [12/20] Train Loss: 0.0003, Acc: 99.99% | Val Loss: 0.0000, Acc: 100.00%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_011_val_100.00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [36:04<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [13/20] Train Loss: 0.0002, Acc: 99.99% | Val Loss: 0.0002, Acc: 99.99%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_012_val_99.99.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [36:31<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [14/20] Train Loss: 0.0003, Acc: 100.00% | Val Loss: 0.0001, Acc: 100.00%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_013_val_100.00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [37:10<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [15/20] Train Loss: 0.0000, Acc: 100.00% | Val Loss: 0.0001, Acc: 100.00%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_014_val_100.00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [37:05<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [16/20] Train Loss: 0.0001, Acc: 100.00% | Val Loss: 0.0000, Acc: 100.00%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_015_val_100.00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [37:06<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [17/20] Train Loss: 0.0001, Acc: 100.00% | Val Loss: 0.0004, Acc: 99.98%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_016_val_99.98.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [37:32<00:00,  1.93s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [18/20] Train Loss: 0.0000, Acc: 100.00% | Val Loss: 0.0001, Acc: 100.00%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_017_val_100.00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [37:45<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [19/20] Train Loss: 0.0000, Acc: 100.00% | Val Loss: 0.0000, Acc: 100.00%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_018_val_100.00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1168/1168 [37:41<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch [20/20] Train Loss: 0.0000, Acc: 100.00% | Val Loss: 0.0000, Acc: 100.00%\n",
      "üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: ./new_model/epoch_019_val_100.00.pt\n",
      "‚úÖ ÌïôÏäµ ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================\n",
    "# ‚öôÔ∏è Í∏∞Î≥∏ ÏÑ§Ï†ï\n",
    "# ===============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "num_workers = 4\n",
    "\n",
    "# ===============================\n",
    "# üß© 1. Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú ÏàòÏßë\n",
    "# ===============================\n",
    "data_dir = \"/mnt/nas/seohan_image/seohan_data/origin/\"\n",
    "labels = ['27_45','27_30']\n",
    "save_dir = './new_model'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "image_paths, image_labels = [], []\n",
    "\n",
    "for label in labels:\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    for img_name in os.listdir(label_dir):\n",
    "            image_paths.append(os.path.join(label_dir, img_name))\n",
    "            image_labels.append(label)\n",
    "\n",
    "# ÎùºÎ≤® Ïù∏Îç±Ïã±\n",
    "label_to_idx = {l: i for i, l in enumerate(sorted(set(image_labels)))}\n",
    "y_all = np.array([label_to_idx[l] for l in image_labels])\n",
    "\n",
    "# ===============================\n",
    "# üß© 2. Stratified Split (80:20)\n",
    "# ===============================\n",
    "train_paths, val_paths, y_train, y_val = train_test_split(\n",
    "    image_paths, y_all, test_size=0.2, stratify=y_all, random_state=42\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# üß† 3. Class Weight Í≥ÑÏÇ∞\n",
    "# ===============================\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(f\"‚úÖ Class Weights: {class_weights}\")\n",
    "\n",
    "# ===============================\n",
    "# üé• 4. Albumentations Ï¶ùÍ∞ï ÏÑ§Ï†ï\n",
    "# ===============================\n",
    "train_transform = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.08, scale_limit=0.05, rotate_limit=10, border_mode=cv2.BORDER_REFLECT_101, p=0.8),\n",
    "    A.MotionBlur(blur_limit=5, p=0.3),\n",
    "    A.GaussianBlur(blur_limit=3, p=0.2),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.CLAHE(p=0.2),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.CenterCrop(height=1900, width=1900),\n",
    "    A.Resize(299, 299),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.CenterCrop(height=1900, width=1900),\n",
    "    A.Resize(299, 299),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# ===============================\n",
    "# üì¶ 5. Dataset Ï†ïÏùò\n",
    "# ===============================\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.paths[idx]).convert(\"RGB\"), dtype=np.float32) / 255.0\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image, label\n",
    "\n",
    "train_dataset = CustomDataset(train_paths, y_train, transform=train_transform)\n",
    "val_dataset = CustomDataset(val_paths, y_val, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# ===============================\n",
    "# üß± 6. timm Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "# ===============================\n",
    "model = timm.create_model(\"inception_resnet_v2\", pretrained=True, num_classes=len(label_to_idx))\n",
    "model = model.to(device)\n",
    "\n",
    "# ===============================\n",
    "# ‚öôÔ∏è 7. ÏÜêÏã§Ìï®Ïàò & ÏòµÌã∞ÎßàÏù¥Ï†Ä\n",
    "# ===============================\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# ===============================\n",
    "# üöÄ 8. ÌïôÏäµ Î£®ÌîÑ\n",
    "# ===============================\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_acc = 100 * correct / total\n",
    "    avg_train_loss = train_loss / total\n",
    "\n",
    "    # -----------------------\n",
    "    # Validation\n",
    "    # -----------------------\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / val_total\n",
    "\n",
    "    print(f\"üìä Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    # ---- Ïä§ÏºÄÏ§ÑÎü¨ Í∞±Ïã† ----\n",
    "    scheduler.step()\n",
    "\n",
    "    # ---- Î™®Îç∏ Ï†ÄÏû• ----\n",
    "    save_path = os.path.join(save_dir, f\"epoch_{epoch:03d}_val_{val_acc:.2f}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, save_path)\n",
    "    print(f\"üíæ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: {save_path}\")\n",
    "\n",
    "    # ---- ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ Î≥ÑÎèÑ Ï†ÄÏû• ----\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pt\"))\n",
    "        print(f\"üåü ÏµúÍ≥† ÏÑ±Îä• Í∞±Ïã†! Best Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"final_model.pt\"))\n",
    "print(\"‚úÖ ÌïôÏäµ ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda35137-011b-43c7-9c3b-e133f58f4594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d5ba5-0df9-410c-a941-9933a1e08c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgn",
   "language": "python",
   "name": "pgn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
